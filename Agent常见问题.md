# RAG检索引擎
1. **标准工作流**
1. 数据准备：清洗与分块->向量化->入库
2. 检索：问题向量化->相似度检索->混合检索（关键词检索（BM25）及向量检索（语义））
3. 重排序：准确率提升关键，Rerank模型对初选出的top20-50等片段进行深度打分，选出top3-5
4. 生成：提示词工程、LLM推理（读资料、写答案）
5. 溯源：回答中标记来源
2. **RAG准确度提升方案**
1. 检索前增强：查询重写、多查询生成、HyDE（假设性文档嵌入）
2. 检索中增强：混合检索、父子分块（检索子块返回父块，保证上下文完整）
3. **检索后增强**：最有效步骤，重排序、上下文压缩（保留召回文本核心语句，减少大模型干扰和节省token）
4. 其他：GraphRAG（图增强检索，搜索全局信息）、Agentic RAG（LLM自我反思和调用工具）
3. **RAG性能及高并发设计**
RAG性能提升本质是多阶段瓶颈优化的过程，链路涉及CPU（业务逻辑）、I/O（数据库检索）、GPU（模型推理）

- 架构设计：解耦与异步化
  - 微服务化与水平扩展：RAG拆为接入层、检索层、重排序层、推理层，独立部署支持自动扩缩容
  - 异步化与流式响应：流式输出、非即时反馈场景通过消息队列削峰填谷
  - 负载均衡：nginx等高性能负载均衡
- 检索层：读写分离与只读副本、索引分片&并行计算（数据量极大）、索引优化（使用HNSW、IVF-PQ索引）、并行混合检索
- 模型工程：Embedding模型、**Rerank模型**、LLM模型，推理加速后端->模型量化与蒸馏->模型网关&多实例调度
- 缓存机制：最高效手段，**语义缓存（语义与历史问题相似）**、传统结果缓存（Embedding结果、Rerank结果等）、预拉取机制（根据用户对话，提前异步加载可能相关的文档片段）
- 性能指标监控：TPOT（Time Per Output Token, 每个token生成速度）、TTFT（Time To First Token, 首字延迟，用户体验核心）、QPS、召回准确率监控

4. **高并发RAG标准链路**
请求进入（负载均衡）-> 语义缓存检查 -> 并行处理（计算Embedding、从Redis获取上下文）-> 检索只读副本 -> Rerank（top50结果GPU加速精排）-> LLM推理（vLLM队列，连续批处理生成答案）-> 流式返回（SSE协议）
5. **RAG快速召回算法，近似最近邻算法-ANN**
- HNSW 算法（分层导航小世界图）：基于图结构，目前最主流
- IVF（倒排文件索引）：利用K-Means算法将向量划为N个簇，根据簇中心距离缩小规模
- PQ（乘积量化）：高维向量划分为子向量并进行压缩（量化），节省内存（10-100倍），支持查表快速估算


# KV存储
1. 核心概念：token（key）-> 向量（value），大模型的短期记忆，本质是在有限的GPU显存中，通过分页管理、压缩量化、共享机制，塞入更多上下文数据。
2. 不可能三角：显存占用、计算精度、访问延迟
3. 解决显存不够的核心优化方案
- PagedAttention（分页注意力机制，vLLM核心）：虚拟内存分页，逻辑上连续，减少碎片化
- 模型架构级优化：多个Query头分组，每组共享一对KV
- 模型量化：将KV向量从FP16压缩至INT8/INT4
- 共享前缀缓存：**RAG或多轮场景高效措施**，基于重复prompt计算KV Cache并在显存中共享
- 卸载：GPU显存达到极限时，将暂时不用的KV Cache迁移至CPU内存甚至SSD，防止程序崩溃，但会导致推理速度下降
